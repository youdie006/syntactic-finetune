#!/usr/bin/env python3
"""
Comprehensive data quality check for syntactic fine-tuning dataset.
"""

import json
import random
from collections import Counter
import os


def comprehensive_quality_check(file_path, sample_size=2000):
    """Perform comprehensive quality check."""
    print(f"\nğŸ” ì¢…í•© í’ˆì§ˆ ê²€ì‚¬: {file_path}")
    print("=" * 70)
    
    if not os.path.exists(file_path):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        return
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    total_lines = len(lines)
    sample_lines = random.sample(lines, min(sample_size, total_lines))
    
    print(f"ì „ì²´ ë¼ì¸: {total_lines:,}")
    print(f"ê²€ì‚¬ ìƒ˜í”Œ: {len(sample_lines):,}")
    print()
    
    # Initialize counters
    issues = {
        'json_parse_errors': 0,
        'missing_messages': 0,
        'wrong_message_count': 0,
        'wrong_roles': 0,
        'empty_content': 0,
        'assistant_json_errors': 0,
        'missing_fields': 0,
        'empty_fields': 0,
        'extremely_long': 0,
        'extremely_short': 0,
        'invalid_tokens': 0
    }
    
    valid_examples = 0
    sentence_lengths = []
    token_counts = []
    
    # Process samples
    for i, line in enumerate(sample_lines):
        try:
            # 1. JSON parsing
            data = json.loads(line.strip())
            
            # 2. Structure validation
            if 'messages' not in data:
                issues['missing_messages'] += 1
                continue
            
            messages = data['messages']
            if len(messages) != 2:
                issues['wrong_message_count'] += 1
                continue
            
            # 3. Role validation
            if messages[0]['role'] != 'user' or messages[1]['role'] != 'assistant':
                issues['wrong_roles'] += 1
                continue
            
            # 4. Content validation
            user_content = messages[0].get('content', '').strip()
            assistant_content = messages[1].get('content', '').strip()
            
            if not user_content or not assistant_content:
                issues['empty_content'] += 1
                continue
            
            # 5. Parse assistant JSON
            try:
                assistant_data = json.loads(assistant_content)
            except json.JSONDecodeError:
                issues['assistant_json_errors'] += 1
                continue
            
            # 6. Check required fields
            required_fields = ['chunks', 'pos_tags', 'grammatical_roles']
            missing_fields = [field for field in required_fields if field not in assistant_data]
            if missing_fields:
                issues['missing_fields'] += 1
                continue
            
            # 7. Check for empty fields
            empty_fields = [field for field in required_fields if not assistant_data[field].strip()]
            if empty_fields:
                issues['empty_fields'] += 1
                continue
            
            # 8. Extract sentence and check length
            sentence = user_content.replace('Analyze this sentence syntactically: ', '')
            sentence_length = len(sentence)
            sentence_lengths.append(sentence_length)
            
            if sentence_length > 500:
                issues['extremely_long'] += 1
            elif sentence_length < 10:
                issues['extremely_short'] += 1
            
            # 9. Token validation (basic)
            try:
                import tiktoken
                encoding = tiktoken.get_encoding("cl100k_base")
                total_tokens = len(encoding.encode(user_content)) + len(encoding.encode(assistant_content))
                token_counts.append(total_tokens)
                
                if total_tokens > 1000:  # Very high token count
                    issues['invalid_tokens'] += 1
            except ImportError:
                pass
            
            valid_examples += 1
            
        except json.JSONDecodeError:
            issues['json_parse_errors'] += 1
        except Exception:
            # Catch any other unexpected errors
            pass
    
    # Print results
    print("ğŸ“Š ê²€ì‚¬ ê²°ê³¼:")
    print(f"âœ… ìœ íš¨í•œ ì˜ˆì œ: {valid_examples:,} ({valid_examples/len(sample_lines)*100:.1f}%)")
    print(f"âŒ ë¬¸ì œê°€ ìˆëŠ” ì˜ˆì œ: {len(sample_lines)-valid_examples:,} ({(len(sample_lines)-valid_examples)/len(sample_lines)*100:.1f}%)")
    print()
    
    print("ğŸ› ë°œê²¬ëœ ë¬¸ì œë“¤:")
    for issue, count in issues.items():
        if count > 0:
            issue_name = {
                'json_parse_errors': 'JSON íŒŒì‹± ì˜¤ë¥˜',
                'missing_messages': 'messages í•„ë“œ ëˆ„ë½',
                'wrong_message_count': 'ë©”ì‹œì§€ ê°œìˆ˜ ì˜¤ë¥˜',
                'wrong_roles': 'ì—­í• (role) ì˜¤ë¥˜',
                'empty_content': 'ë¹ˆ ë‚´ìš©',
                'assistant_json_errors': 'ì–´ì‹œìŠ¤í„´íŠ¸ JSON ì˜¤ë¥˜',
                'missing_fields': 'í•„ìˆ˜ í•„ë“œ ëˆ„ë½',
                'empty_fields': 'ë¹ˆ í•„ë“œ',
                'extremely_long': 'ê·¹ë„ë¡œ ê¸´ ë¬¸ì¥',
                'extremely_short': 'ê·¹ë„ë¡œ ì§§ì€ ë¬¸ì¥',
                'invalid_tokens': 'ë¹„ì •ìƒì  í† í° ìˆ˜'
            }.get(issue, issue)
            print(f"  - {issue_name}: {count}")
    
    if sentence_lengths:
        print(f"\nğŸ“ ë¬¸ì¥ ê¸¸ì´ í†µê³„:")
        print(f"  - í‰ê· : {sum(sentence_lengths)/len(sentence_lengths):.1f}ì")
        print(f"  - ìµœì†Œ: {min(sentence_lengths)}ì")
        print(f"  - ìµœëŒ€: {max(sentence_lengths)}ì")
        print(f"  - ì¤‘ê°„ê°’: {sorted(sentence_lengths)[len(sentence_lengths)//2]}ì")
    
    if token_counts:
        print(f"\nğŸ”¤ í† í° í†µê³„:")
        print(f"  - í‰ê· : {sum(token_counts)/len(token_counts):.1f}")
        print(f"  - ìµœì†Œ: {min(token_counts)}")
        print(f"  - ìµœëŒ€: {max(token_counts)}")
    
    # Calculate overall quality score
    quality_score = valid_examples / len(sample_lines) * 100
    print(f"\nğŸ† ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}%")
    
    if quality_score >= 95:
        print("   ğŸŸ¢ ìš°ìˆ˜í•¨ - ë°ì´í„° í’ˆì§ˆì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤")
    elif quality_score >= 85:
        print("   ğŸŸ¡ ì–‘í˜¸í•¨ - ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤")
    elif quality_score >= 70:
        print("   ğŸŸ  ë³´í†µ - ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
    else:
        print("   ğŸ”´ ë¬¸ì œ ìˆìŒ - ìƒë‹¹í•œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
    
    return quality_score


def check_tag_consistency(file_path, sample_size=1000):
    """Check consistency of tags across the dataset."""
    print(f"\nğŸ·ï¸ íƒœê·¸ ì¼ê´€ì„± ê²€ì‚¬")
    print("=" * 70)
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    sample_lines = random.sample(lines, min(sample_size, len(lines)))
    
    pos_tags = set()
    chunk_categories = set()
    role_categories = set()
    
    for line in sample_lines:
        try:
            data = json.loads(line)
            messages = data['messages']
            assistant_content = json.loads(messages[1]['content'])
            
            # Collect POS tags
            pos_list = assistant_content['pos_tags'].split()
            pos_tags.update(pos_list)
            
            # Collect chunk categories
            chunks = assistant_content['chunks']
            for chunk in chunks.split(']'):
                if '[' in chunk:
                    category = chunk.split('[')[1].split(' ')[0] if ' ' in chunk else chunk.split('[')[1]
                    chunk_categories.add(category)
            
            # Collect role categories
            roles = assistant_content['grammatical_roles'].split(' | ')
            for role in roles:
                if ':' in role:
                    category = role.split(':')[0]
                    role_categories.add(category)
                    
        except Exception:
            continue
    
    print(f"ğŸ”¤ ê³ ìœ  POS íƒœê·¸ ìˆ˜: {len(pos_tags)}")
    print(f"   ê°€ì¥ ë¹ˆë²ˆí•œ 10ê°œ: {', '.join(list(pos_tags)[:10])}")
    
    print(f"\nğŸ·ï¸ ê³ ìœ  ì²­í¬ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(chunk_categories)}")
    print(f"   ì˜ˆì‹œ: {', '.join(list(chunk_categories)[:10])}")
    
    print(f"\nâš™ï¸ ê³ ìœ  ë¬¸ë²• ì—­í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(role_categories)}")
    print(f"   ì˜ˆì‹œ: {', '.join(list(role_categories)[:10])}")


def final_recommendations(quality_scores):
    """Provide final recommendations based on quality analysis."""
    print(f"\nğŸ“‹ ìµœì¢… ê¶Œì¥ì‚¬í•­")
    print("=" * 70)
    
    avg_quality = sum(quality_scores) / len(quality_scores)
    
    print(f"ì „ì²´ í‰ê·  í’ˆì§ˆ: {avg_quality:.1f}%")
    print()
    
    if avg_quality >= 95:
        print("âœ… ë°ì´í„°ì…‹ì´ íŒŒì¸íŠœë‹ì— ì í•©í•©ë‹ˆë‹¤.")
        print("   - ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        print("   - ë°”ë¡œ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    elif avg_quality >= 85:
        print("âœ… ë°ì´í„°ì…‹ì´ íŒŒì¸íŠœë‹ì— ì í•©í•©ë‹ˆë‹¤.")
        print("   - ì†Œìˆ˜ì˜ ë¬¸ì œê°€ ìˆì§€ë§Œ ì „ì²´ì ìœ¼ë¡œ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        print("   - íŒŒì¸íŠœë‹ ì§„í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    elif avg_quality >= 70:
        print("âš ï¸ ë°ì´í„°ì…‹ì— ì¼ë¶€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("   - ê°€ëŠ¥í•˜ë©´ ë¬¸ì œ ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        print("   - í˜„ì¬ ìƒíƒœë¡œë„ íŒŒì¸íŠœë‹ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("âŒ ë°ì´í„°ì…‹ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("   - íŒŒì¸íŠœë‹ ì „ì— ë°˜ë“œì‹œ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    print(f"\nğŸ“ˆ ì˜ˆìƒ íŒŒì¸íŠœë‹ ë¹„ìš©:")
    print(f"   - í›ˆë ¨ ë°ì´í„°: ~81K ì˜ˆì œ")
    print(f"   - ê²€ì¦ ë°ì´í„°: ~15K ì˜ˆì œ") 
    print(f"   - ì˜ˆìƒ ë¹„ìš©: $150-200 (í† í° ìˆ˜ ê¸°ì¤€)")


def main():
    print("ğŸ” Syntactic Dataset Quality Check")
    print("=" * 70)
    
    files = [
        'data_final/train.jsonl',
        'data_final/valid.jsonl', 
        'data_final/test_local.jsonl'
    ]
    
    quality_scores = []
    
    # Check each file
    for file_path in files:
        if os.path.exists(file_path):
            score = comprehensive_quality_check(file_path, 1000)
            quality_scores.append(score)
    
    # Check tag consistency on training data
    if os.path.exists('data_final/train.jsonl'):
        check_tag_consistency('data_final/train.jsonl', 500)
    
    # Final recommendations
    if quality_scores:
        final_recommendations(quality_scores)
    
    print(f"\nâœ… í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()