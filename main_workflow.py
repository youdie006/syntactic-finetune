#!/usr/bin/env python3
"""
Main Workflow - ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ë‘ ê°€ì§€ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ì›:
1. í…ìŠ¤íŠ¸ â†’ CSV ìƒì„± (data_generation)
2. CSV â†’ JSONL ë³€í™˜ (fine_tuning)
"""

import argparse
import sys
import os
import subprocess
from pathlib import Path

# ê³µí†µ ìœ í‹¸ë¦¬í‹° ì¶”ê°€
sys.path.append('shared')
from common_utils import validate_csv_format, calculate_dataset_stats


def main():
    parser = argparse.ArgumentParser(description='Syntactic Analysis Workflow Manager')
    subparsers = parser.add_subparsers(dest='workflow', help='Available workflows')
    
    # í…ìŠ¤íŠ¸ â†’ CSV ì›Œí¬í”Œë¡œìš°
    csv_parser = subparsers.add_parser('generate-csv', help='Generate CSV from text')
    csv_parser.add_argument('input_file', help='Input text file')
    csv_parser.add_argument('--output', '-o', help='Output CSV file path')
    csv_parser.add_argument('--translation-file', help='Translation file (optional)')
    csv_parser.add_argument('--validate', action='store_true', help='Validate output CSV')
    
    # CSV â†’ JSONL ì›Œí¬í”Œë¡œìš°
    jsonl_parser = subparsers.add_parser('generate-jsonl', help='Generate JSONL from CSV')
    jsonl_parser.add_argument('csv_file', help='Input CSV file')
    jsonl_parser.add_argument('--experiment-name', required=True, help='Experiment name')
    jsonl_parser.add_argument('--categories', type=int, help='Number of categories for dynamic strategy')
    jsonl_parser.add_argument('--strategy', help='Strategy name (baseline, simplified, detailed, frequency_based)')
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸
    pipeline_parser = subparsers.add_parser('full-pipeline', help='Full text â†’ CSV â†’ JSONL pipeline')
    pipeline_parser.add_argument('input_file', help='Input text file')
    pipeline_parser.add_argument('--experiment-name', required=True, help='Experiment name')
    pipeline_parser.add_argument('--categories', type=int, default=5, help='Number of categories')
    pipeline_parser.add_argument('--csv-output', help='Intermediate CSV file path')
    
    # ìƒíƒœ í™•ì¸
    status_parser = subparsers.add_parser('status', help='Check system status and requirements')
    
    # í†µê³„ ì •ë³´
    stats_parser = subparsers.add_parser('stats', help='Show dataset statistics')
    stats_parser.add_argument('csv_file', help='CSV file to analyze')
    
    args = parser.parse_args()
    
    if not args.workflow:
        parser.print_help()
        return
    
    try:
        if args.workflow == 'generate-csv':
            handle_csv_generation(args)
        elif args.workflow == 'generate-jsonl':
            handle_jsonl_generation(args)
        elif args.workflow == 'full-pipeline':
            handle_full_pipeline(args)
        elif args.workflow == 'status':
            handle_status_check()
        elif args.workflow == 'stats':
            handle_stats(args)
            
    except Exception as e:
        print(f"âŒ Error: {e}")
        sys.exit(1)


def handle_csv_generation(args):
    """í…ìŠ¤íŠ¸ â†’ CSV ìƒì„± ì²˜ë¦¬"""
    print("ğŸ”„ Starting CSV generation workflow...")
    
    # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    if not args.output:
        input_name = Path(args.input_file).stem
        args.output = f"data_generation/output/{input_name}_analysis.csv"
    
    # CSV ìƒì„± ëª…ë ¹ ì‹¤í–‰
    cmd = [
        sys.executable,
        "data_generation/generate_csv.py",
        "batch",
        args.input_file,
        "--output", args.output
    ]
    
    if args.translation_file:
        cmd.extend(["--translation-file", args.translation_file])
    
    print(f"Executing: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… CSV generation completed!")
        print(result.stdout)
        
        # ê²€ì¦ ìˆ˜í–‰
        if args.validate:
            print("ğŸ” Validating CSV format...")
            if validate_csv_format(args.output):
                print("âœ… CSV validation passed")
            else:
                print("âŒ CSV validation failed")
                
    else:
        print("âŒ CSV generation failed!")
        print("STDERR:", result.stderr)


def handle_jsonl_generation(args):
    """CSV â†’ JSONL ìƒì„± ì²˜ë¦¬"""
    print("ğŸ”„ Starting JSONL generation workflow...")
    
    # CSV íŒŒì¼ ê²€ì¦
    if not validate_csv_format(args.csv_file):
        raise ValueError(f"Invalid CSV format: {args.csv_file}")
    
    # fine_tuning ë””ë ‰í† ë¦¬ë¡œ ì´ë™í•˜ì—¬ ì‹¤í–‰
    cmd = [
        sys.executable,
        "fine_tuning/run_experiment.py",
        "run",
        "--name", args.experiment_name,
        "--input", args.csv_file
    ]
    
    # ì „ëµ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ ì„¤ì •
    if args.categories:
        cmd.extend(["--categories", str(args.categories)])
    elif args.strategy:
        cmd.extend(["--strategy", args.strategy])
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ 5ê°œ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
        cmd.extend(["--categories", "5"])
    
    print(f"Executing: {' '.join(cmd)}")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… JSONL generation completed!")
        print(result.stdout)
    else:
        print("âŒ JSONL generation failed!")
        print("STDERR:", result.stderr)


def handle_full_pipeline(args):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬"""
    print("ğŸš€ Starting full pipeline: Text â†’ CSV â†’ JSONL")
    
    # 1ë‹¨ê³„: CSV ìƒì„±
    print("\nğŸ“ Step 1: Generating CSV from text...")
    
    if not args.csv_output:
        input_name = Path(args.input_file).stem
        args.csv_output = f"results/generated_csv/{input_name}_{args.experiment_name}.csv"
    
    # CSV ìƒì„± ì‹¤í–‰
    csv_args = argparse.Namespace(
        input_file=args.input_file,
        output=args.csv_output,
        translation_file=None,
        validate=True
    )
    handle_csv_generation(csv_args)
    
    # 2ë‹¨ê³„: JSONL ìƒì„±
    print("\nğŸ”„ Step 2: Generating JSONL from CSV...")
    
    jsonl_args = argparse.Namespace(
        csv_file=args.csv_output,
        experiment_name=args.experiment_name,
        categories=args.categories,
        strategy=None
    )
    handle_jsonl_generation(jsonl_args)
    
    print("\nğŸ‰ Full pipeline completed successfully!")
    print(f"ğŸ“Š Generated CSV: {args.csv_output}")
    print(f"ğŸ“ JSONL datasets available in: fine_tuning/data_experiments/")


def handle_status_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    print("ğŸ” Checking system status...")
    
    # Python ë²„ì „ í™•ì¸
    print(f"Python version: {sys.version}")
    
    # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    required_packages = ['pandas', 'spacy', 'pyyaml']
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package}: installed")
        except ImportError:
            print(f"âŒ {package}: not installed")
    
    # spaCy ëª¨ë¸ í™•ì¸
    try:
        import spacy
        nlp = spacy.load("en_core_web_sm")
        print("âœ… spaCy English model: available")
    except OSError:
        print("âŒ spaCy English model: not installed")
        print("   Install with: python -m spacy download en_core_web_sm")
    
    # ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸
    required_dirs = [
        'data_generation/src',
        'data_generation/output',
        'fine_tuning/src',
        'fine_tuning/configs',
        'shared',
        'results/generated_csv'
    ]
    
    for dir_path in required_dirs:
        if Path(dir_path).exists():
            print(f"âœ… Directory: {dir_path}")
        else:
            print(f"âŒ Directory missing: {dir_path}")
    
    print("\nğŸ“‹ Status check completed")


def handle_stats(args):
    """ë°ì´í„°ì…‹ í†µê³„ í‘œì‹œ"""
    print(f"ğŸ“Š Analyzing dataset: {args.csv_file}")
    
    if not Path(args.csv_file).exists():
        raise FileNotFoundError(f"CSV file not found: {args.csv_file}")
    
    stats = calculate_dataset_stats(args.csv_file)
    
    print(f"\nğŸ“ˆ Dataset Statistics:")
    print(f"  Total sentences: {stats['total_sentences']:,}")
    print(f"  Unique sentences: {stats['unique_sentences']:,}")
    print(f"  Average sentence length: {stats['avg_sentence_length']:.1f} characters")
    print(f"  Min/Max sentence length: {stats['min_sentence_length']}/{stats['max_sentence_length']} characters")
    print(f"  Sentences with translations: {stats['has_translations']:,}")
    print(f"  Average tags per sentence: {stats['avg_tags_per_sentence']:.1f}")
    print(f"  Total unique tags: {stats['total_unique_tags']:,}")


if __name__ == "__main__":
    main()